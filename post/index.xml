<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Offliner&#39;s Blog</title>
    <link>https://Offliners.github.io/post/</link>
    <description>Recent content in Posts on Offliner&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sat, 19 Jun 2021 14:45:47 +0800</lastBuildDate><atom:link href="https://Offliners.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NTU Machine Learning Week 17 - Network Compression</title>
      <link>https://Offliners.github.io/post/ntuml-week17-2/</link>
      <pubDate>Sat, 19 Jun 2021 14:45:47 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week17-2/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2
Youtube Video Link (English): Video 1 Video 2
Why we need network compression 為什麼需要將大模型簡化成小模型並表現差不多呢?因為很多時候需要將模型訓練在資源有限的環境下，如智能手錶、IoT裝置等，那為什麼不將資料傳回雲端訓練再傳回去呢?常見的理由是為了低延遲與隱私
Network Pruning 顧名思義就是修剪模型中的參數，因為大模型中不一定每個參數都有運算到，所以找出無用的參數再修剪掉
首先訓練一個大的模型，再去評估每個參數的重要性，例如weight可以使用絕對值來看其大小，Neuron可以判斷輸出不為0的次數等，評估後再修剪一些參數，通常修剪後正確率會掉一些，為了能回升回來通常會Fine-tune，Fine-tune後還能再評估個參數的重要性，這步驟可以執行多次值到模型你覺得夠小為止。那為何不一次修剪多個無用的參數呢?因為根據文獻，一次修剪過多會對模型造成極大損傷，可能無法復原回來
先剪可以用Weight為單位或者用Neuron，若以Weight為單位的話，將無用的Weight去掉後形狀會變成不規則(如上右圖)，會變得不好實作，即使時做出來也會不好加速運算，因為加速運算是透過矩陣運算來實現，不規則形狀會使得運算難以平行化，若為了維持形狀可能會將值變成0，但這樣模型並沒有變小
根據文獻可以發現即使修剪了95%左右的參數，結果在多數情況下訓練沒有加速
因此使用Neuron Pruning會比較好實作，因為修剪掉Neuron仍可以維持好模型的形狀
那有人可能問大模型與小模型有差不多的表現，那為什麼不直接訓練小模型就好呢?訓練大模型再Pruning變小太麻煩。理由是因為大模型比較好訓練，訓練小模型表現往往不好
若想了解為何大模型比較好訓練可以參考以下連結 :
 Geometry of Loss Surfaces (Conjecture) : Video The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks : Link  接下透過大樂透假說來解釋為何大模型比較好訓練，由於模型的表現與Initial weight有關，若初始點好可能可以訓練一組好的參數。將大模型看成多個小模型的組合，透過訓練多個小模型來提高取得好參數的機率，只要一個小模型表現好，那麼大模型就會好
大樂透假說的證實方式與Pruning有關，將大模型初始化參數後進行訓練，訓練後進行Pruning變成小模型，小模型若再重新初始化一次參數再訓練會發現訓練不起來，但小模型使用大模型的初始化參數卻可以成功訓練。因此使用大樂透假說來解釋，這個Pruning後的模型就是大模型中多個小模型裡表現最好的小模型，若只訓練小模型運氣不好的話就訓練不起來
大樂透假說提出後也有許多相關的研究，如上圖有人使用多種Pruning的方法來實驗，發現Pruning前與Pruning後的絕對值差越大越有效。另一個有趣的結果是訓練小模型的初始參數只要與大模型初始參數有相同的正負號就能訓練起來，不論值的大小。最後一個神奇的發現是雖然是給與隨機的參數，但在大模型中已經存在一個小模型，只要套用這組隨機參數就能有好表現了
若想了解更多可以參考以下連結 :
 Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask : Link Weight Agnostic Neural Networks : Link  那大樂透假說是對的嗎?</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 17 - Life-long Learning</title>
      <link>https://Offliners.github.io/post/ntuml-week17-1/</link>
      <pubDate>Sat, 19 Jun 2021 14:44:06 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week17-1/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2
Youtube Video Link (English): Video 1 Video 2
Life Long Learning in real-world applications Life Long Learning在現實生活中可以應用在線上系統，當用戶用完後提供回饋，讓模型有新的資料與新任務繼續訓練，使模型越來越強大。那Life Long Learning看起來只是訓練更多的資料，會有什麼難點呢?
目前的Life Long Learning還沒達到先學習影音辨識，再學習影像辨識，接著翻譯任務這樣多重任務的水平，因此雖說是兩個任務，是指兩個類似的任務但資料Domain不同而已。首先將模型來學習任務一，完成後在拿相同的模型與參數來繼續學習任務二，但可以發現學習完任務二會忘記任務一。也許有人覺得模型太小，學習能力有限
但如果將任務一與任務二結合讓相同架構的模型學習，可以發現模型是有能力學習的。因此，模型要學會兩個任務需要一起學習才可以
接下來用QA任務來實驗看看，這裡使用簡單的QA任務，是早期Facebook提供的資料集，裡面包含了20個任務
上圖圖表橫軸是對應的任務，縱軸是在任務五的準確率。可以發現在任務五前準確率是0，因為還沒學習到任務五，當看過任務五時準確率100%，但當學習任務五後學習新任務準確率卻暴跌
有人可能懷疑模型本身沒有能力能多學這麼多任務，但如果將所有任務讓機器同時學可以發現模型是有能力的。這樣的情況被稱為Catastrophic Forgetting
有些人可能會想說那只要把多個任務組合起來就好了，為什麼要捨棄前任務讓機器學習新任務呢?使用Multi-task Training就能解決了吧?假設要學習第1000個任務又不忘記前面的任務的話，首先就須要有足夠的容量來儲存先前的任務，再來是一次學習1000個任務需要大量的時間來學習。在許多Life Long Learning研究中，會把Multi-task Training當成是上限，也就是先使用Multi-task Training當成上限，再使用Life Long Learning來逼近這界線
接下來可能有人會疑惑為什麼不把任務切開分別讓每個模型學習呢?為何要堅持一個模型學多個任務?使用每個模型學習單一任務確實不會有Catastrophic Forgetting的問題，但當任務量眾多時，也要要考慮是否有足夠的容量能儲存這些模型。Life Long Learning想了解的是，人類一個腦可以學習多件事，那要如何讓機器也能做到一樣的事
有些人可能會覺得這不就是Transfer Learning嗎?但其實關注點不同，Transfer Learning是學習好第一個任務，在第二任務表現如何，而Life Long Learning是第一個任務學完後再學第二個任務，第二個任務學完後在第一個任務表現如何
Evaluation 要評估Life Long Learning的模型好不好，首先需要有一系列的任務，通常會將同一個資料集透過一些規則來打亂來產生新任務，或者把類別分別拆出來成新任務
假設有T個任務，首先會有T個隨機初始化的參數用來分別學習各個任務得到T個準確率，再來讓模型先學第一個任務，學完後接續學後續的任務並記錄各任務的準確率，以此類推到第T個任務的準確率都記錄下來後可以得到一個表格，這個表格可以用賴判斷Life Long Learning的模型好不好。如果看i &amp;gt; j的部分，表示看這模型是否忘記先前的任務，若看i &amp;lt; j的話，看模型在學到該任務前是否無思自通，學會之後的任務，也就是機器Transfer的能力。通常評估的方法是將最後一欄的準確率取平均，還有另一個評估方式是Backward Transfer，計算學完該任務以及學到最後時的準確率差多少，最後取平均，可以用來看模型遺忘的程度有多嚴重，因此這指標通常是負值</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 16 - Reinforcement Learning</title>
      <link>https://Offliners.github.io/post/ntuml-week16/</link>
      <pubDate>Fri, 11 Jun 2021 17:14:52 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week16/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2 Video 3 Video 4 Video 5
Youtube Video Link (English): Video 1 Video 2 Video 3 Video 4 Video 5
What is RL ? 機器學習最主要在做的事情就是找一個Function，Reinforcement Learning也是機器學習的一種，所以它也是在找一個Function。在RL中會有個Actor與Environment，首先Environment會給Actor一個Observation，也就是Actor的輸入，接著Actor會有輸出Action去影響Environment來產生新的Observation，互動期間Environment會給Actor一些Reward來告訴Actor這樣的Action是否是好的。所以Actor其實就是我們要找的Function，它要能讓Actor得到Reward的總和是最大的
Example: Playing Video Game 接下來使用Space invader來舉例，遊戲玩法是開火擊退所有外星人，而玩家有護盾可以抵擋外星人的攻擊(玩家的攻擊也會破壞護盾)，擊退外星人可以獲得分數。遊戲終止有兩個情況，一個是退所有外星人，另一個情況是玩家被外星人擊毀
當要訓練RL來玩這遊戲的話，Actor就是玩家，Environment是遊戲本身，Observation是遊戲畫面，Action是輸出，有三種可能的行為分別為向左向右還有開火。如果Action選擇向右的話，reward是0因為沒有擊退外星人
當遊戲畫面變化時，表示有新的Observation，這時就會有新的Action，如果選擇開火並擊退外星人的話，就會得到reward等於5。因此學習的目標是，讓Actor能得到的reward總和是最大的
Example: Learning to play Go 若是Alpha Go的話，Environment是人類對手，Observation就是棋盤，Action就是落子的位置，透過對手的落子來產生新的Observation，然後持續下去
在下圍棋中，任何行為都沒辦法得到reward，因此會定義贏了才能得到reward為1，輸了得到-1分
How to train RL ? 那要怎麼訓練RL呢?與先前的相同，第一步是有未知數的Functin，這些未知數是要被找出來的，第二步是定義loss function，第三步是最小化loss
首先是第一步，會使用Network，輸入是遊戲畫面，輸出是每個行為的分數，這件事情其實就是分類。Network架構可以自行設計，若輸入是遊戲畫面可能會使用CNN，若想看遊戲開始到現在發生的情況就有可能使用RNN或者Transformer。最後機器會採取什麼Action取決於輸出的分數，通常會轉換成機率並隨機Sample，那為什麼不直接選擇機率最高的呢?多數RL的應用中，會選擇隨機Sample，這樣子即使看到相同畫面也有可能採取不同行為，來增加隨機性
第二步要定義Loss，在Environment與Actor的互動中會得到許多reward直到遊戲結束，從遊戲開始到結束叫一個Episode，將這些reward總合起來叫Total reward或者Return，這是我們要去最大的目標，加上負號就是loss，也就是要最小化
最後是第三步，Environment會產生Observation叫s1，再來將s1輸入給Actor產生Action叫a1，持續下去到遊戲終止，這些s與a形成的Sequence叫做Trajectory。得到的Reward能當成是一個Function，有些遊戲只看Action就能決定，但通常會看當下的Observation，像Space invader需要觀察Observation是否擊退敵人才能判斷是否得到reward。但這不是一般的Optimization問題，首先是Actor的輸出是有隨機性，而且Environment是個黑盒子，只知道輸入後會產生回應，但對應關係我們不知道。因此在RL中解決Optimization問題使用的方法會與之前不同
Policy Gradient 首先思考要Actor在輸入Observation後如何輸出Action，先前提到這是個分類問題，當輸入某個Observation時希望能輸出向左，那麼則透過Cross-entropy來最小化loss就好，若希望不要向左則加負號即可，因此只要有適當的Label與loss，就可以控制Actor做我們想做的行為</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 14 - Domain Adaptation</title>
      <link>https://Offliners.github.io/post/ntuml-week14/</link>
      <pubDate>Mon, 24 May 2021 15:33:26 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week14/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video
Youtube Video Link (English): Video
Domain Shift 到目前為止已經知道如何訓練許多模型，且大多能獲得極高的正確率，但當測試資料的分布與訓練資料不同那該怎麼辦? 舉訓練手寫數字為例，如果測試資料圖片的背景不再是黑白而是彩色的，可以發現準確率會低很多。這種測試與訓練資料分布不同的叫做Domain Shift，要解決這種問題就需要Domain adaptation的技術，這種技術可以看成是一種Transfer learning
若對Transfer learning感興趣的可以看此連結影片 :
 ML Lecture 19: Transfer Learning : Video  Domain Shift除了輸入分布有變化以外，輸出的分布也可能有變化，可能特定類別輸出的機率特別大，甚至還有一種罕見的可能是輸入輸出分布相同，但關係產生了變化，可能訓練資料的標籤是0，但測試資料標籤變成1。接下來介紹會著重在輸入資料不同的方面，來自訓練資料的會稱為Source domain，來自測試資料的會稱為Target domain
Domain Adaptation 接下來舉例會需要Domain Adaptation的情境，首先會有一堆訓練資料訓練出來的模型，但我們會希望這模型能應用在不同的domain上，因此訓練時會需要這模型對不同的domain有一些的了解，通常會需要一點另一個domain上些許有標記的資料，但通常Target domain的資料量非常少，因此要小心不要overfit
而更常見的情境是擁有大量Target domain的資料，但都沒有標註，那要怎麼用這些沒有標註的資料在Source domain訓練出一個模型可以用在Target domain上呢?
基本的想法是訓練一個Feature Extractor來把不同的特徵濾掉，留下Source domain與Target domain共同的特徵，這樣就會有相同的分布，這些特徵可以幫助我們在Source domain訓練一個模型並可以套用在Target domain，那要怎麼得到Feature Extractor呢?
一個Classifier可以看成兩個部分，一部分是Feature Extractor，另一部分是Label Predictor，對於Source domain的資料，訓練輸入的資料與得到的輸出越接近越好，但問題是Target domain沒有標註，所以要比較兩個domain在Feature Extractor後輸出的分布是否類似，為了能讓兩分布越接近，因此要使用Domain Adversarial Training的技術
Domain Adversarial Training就是訓練一個二元分類器，輸入這些分布後判斷來自Source domain還是Target domain，所以Feature Extractor有額外的任務要Source domain的label能騙過Domain Classifier，這樣的步驟就像是GAN。Feature Extractor要騙過Domain Classifier只要輸出0就好，但因為Label Predictor需要Feature Extractor來輸出預測的標註，所以Feature Extractor就不會輸出0</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 13 - Explainable AI</title>
      <link>https://Offliners.github.io/post/ntuml-week13/</link>
      <pubDate>Sun, 16 May 2021 03:42:10 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week13/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2
Youtube Video Link (English): Video 1 Video 2
Why we need Explainable ML? 模型得到正確答案不代表模型很聰明，因此在將來，模型得到正確答案時需要解釋為什麼得到這個答案。以上為現今機器學習應用的舉例，如銀行借貸款、醫療診斷、法律判決與自駕車等，模型需要為得出的答案給出合理的解釋才能使人信服
Interpretable v.s. Powerful 若採用較好解釋的模型如Linear model，但往往表現較差;若使用Deep model較難解釋，但表現較好。因此有人認為不應該使用Deep model，因為它是個黑盒子，但這有點削足適履，應該想辦法去提升模型的解釋力
可能有人會提那使用Decision tree? Decision tree相較Linear model強大，也比Deep model好解釋，透過節點問題分析來選擇最終決斷
但一棵Decision tree也可以有複雜的結構(左圖)，而且實際上也不會只使用一棵，而是Random forest的技術(右圖)，使Decision tree變得複雜難以解釋
Goal of Explainable ML 以往模型訓練都有個明確的目標，如降低loss、提升accuracy，那Explainable ML是什麼呢?其實Explainable ML目標非常不明確，許多人會覺得Explainable ML要能闡述模型的一切，模型是怎麼做的，怎麼得到這樣的結果，來打開Deep network這個黑盒子，因為很多人認為Deep network是個黑盒子不能相信，但其實現實生活中充滿著許多黑盒子，如人腦，人腦也是個黑盒子但我們可以相信另一個人的決策，那為什麼不能相信Deep network?也許就差在Deep network沒給出一個令人信服的理由。這裡舉了一個哈佛大學做的一個實驗，大家在排隊等印表機時，有人插隊的理由是只有5頁要印，有60%的人會同意，如果理由是趕時間時，同意的人提高到94%，但神奇的是如果理由是他必須要先印，同意的人也能高達到93%
因此Explainable ML的目標就是要給出使人信服的理由，對象可能是你的客戶、老闆甚至是自己
Explainable ML Explainable ML分成兩大類，一類是Local Explanation，比如影像辨識說這張圖片是一隻貓，那機器要回答說為什麼這張圖片是一隻貓，根據這張圖片來回答。另一個是Global Explanation，要問機器說什麼樣的圖片他會覺得是一隻貓，並不是針對任一張圖片
Local Explanation : Explain the Decision 首先講述Local Explanation，一張圖片機器覺得是貓是根據什麼?</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 11 - Adversarial Attack</title>
      <link>https://Offliners.github.io/post/ntuml-week11/</link>
      <pubDate>Sat, 15 May 2021 20:36:57 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week11/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2
Youtube Video Link (English): Video 1 Video 2
Motivation 要把訓練的模型應用在現實中，光是正確率高是沒用的，還需要應付人類惡意的攻擊，對於這些惡意行為要能偵測出來
How to Attack 首先講如何攻擊，假設在做影像辨識，將未受到攻擊的圖片(Benign Image)輸入製模型中，模型能判別說這是貓，那將未受攻擊的圖片加入一些雜訊(通常很小)變成受攻擊的圖片(Attacked Image)，模型則會判別成其他東西，若只是使模型判別變成其他類別是Non-targeted attack，若可以使模型誤判成特並的類別則是Targeted attack
用ResNet50這個有名的模型來做示範，Benign Image輸入時能辨識成貓咪並有0.64的信心分數，但加入一個人們無法辨視的雜訊後再輸入製模型，發現被辨識成海星，且信心分數達100%
為了證明兩圖片是不同的，將兩張圖片做相減並放大50倍，可以發現雜訊
甚至也能加入其他雜訊使貓咪被辨識成鍵盤
至於實際上是怎麼加入雜訊的?首先舉Non-targeted為例，將隨機雜訊加入圖片丟入模型產生的輸出要與預期輸出的差值越大越好，影像辨識的Loss function常使用Cross-entropy，所以在前面加上負號，當這個值越小表示，預測值與實際值差越大
對於Targeted，加入雜訊的圖片輸入模型得到的輸出值，除了要與預期輸出的差值越大越好，還有與目標值越小越好，因此Loss function會再加上輸出值與目標的Cross-entropy。不管對Non-targeted還是Targeted，接下來都是解這個Optimization的問題，但除了解這問題之外還會加一個限制，為了使加入雜訊的圖片與原圖片看起來非常相似，所以加入的限制就是原圖片與加入雜訊的圖片的差值需要小於某個值，這個值是根據人類的感知來決定
常見有兩個差值的計算，第一種是L2-norm，就是將圖片每一維的差值取平方相加，另一種是L-infinity，將每一維差值取絕對值，並將最大的當輸出值。至於哪種在攻擊中比較好，假設有張只有4個pixel的圖片，做兩個處理，一個是每個pixel都改一點點，另一個是在右下角的pixel改非常多，這兩種都擁有相同的L2-norm，但在L-infinity卻不同，因此L-infinity比較符合人類的感知能力，但目前是舉影像辨識，若是做聲音訊號的攻擊，則需要研究哪種比較符合人類的聽覺系統
至於要怎麼解Optimization問題?與先前相同，只是改變的不是模型內參數而是輸入的x，先不考慮限制，初始值也不是隨機給而是原始圖片，接著開始計算Gradient到結束。那考慮限制呢?
考慮限制的話，在更新參數後發現不符合限制，則修改更新後的參數，假設使用L-infinity，則x只能在藍色方形中，若在範圍外則在藍色框框上找距離範圍外的點最近的點就好
FGSM 接下來介紹一個簡單的攻擊方法Fast Gradient Sign Method (FGSM)，它只需更新一次參數，但在梯度上有個特殊的設計，對梯度每個維度皆取Sign，若大於0則輸出1，小於0輸出-1，接著再乘上現制的值就是更新後的參數，這些參數會落在藍色框框的四個角上
想了解更多FGSM可參考以下連結 :
 Explaining and Harnessing Adversarial Examples : Link  IFGSM 若FGSM多做幾次就是Iterative FGSM(IFGSM)，但有可能會跑出限制外，所以需將跑出的參數修正回來
White Box v.s. Black Box 先前介紹的都是White Box Attack，因為需要知道模型的參數才能計算Gradient，所以對於未知模型參數(如線上API服務)的攻擊就是Black Box Attack</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 10 - Auto-Encoder</title>
      <link>https://Offliners.github.io/post/ntuml-week10/</link>
      <pubDate>Sat, 01 May 2021 18:21:00 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week10/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2
Youtube Video Link (English): Video 1 Video 2
Self-supervised Learning Framework 將一些沒有標註的資料給模型訓練叫Self-supervised Learning或者Pre-train，訓練完後的模型做些微調整後可以應用在其他任務上。而Auto-encoder也是不需要標註資料的模型，因此也是一種Self-supervised Learning
Auto-encoder 在Auto-encoder中有兩個network，一個是Encoder，另一個是Decoder。假設有一堆沒有標註的圖片，會輸入到Encoder中輸出成向量，這個向量再輸入至Decoder，輸出成一張圖片，Decoder運作類似GAN中的Generator。訓練目標是希望Decoder輸出的圖片與輸入Encoder的圖片越接近越好，而這件事叫做Reconstruction，行為類似Cycle GAN。中間的向量有人稱為Embedding、Representation或Code。通常Encoder的輸入會是高維度的向量，而Encoder的輸出會是低維度的向量，所以Encoder做的事就是Dimension Reduction
若想了解更多Dimension Reduction技術可以參考以下連結 :
 Unsupervised Learning - Linear Methods : Video Unsupervised Learning - Neighbor Embedding : Video  Why Auto-encoder? Auto-encoder中的Encoder能將複雜的圖片變成較簡單向量來表示
Auto-encoder的想法已經持續許久，早在2006就已經有了
De-noising Auto-encoder De-noising Auto-encoder會將輸入圖片加入一些雜訊，試圖讓Auto-encoder還原沒有雜訊的圖片，讓模型學會濾除雜訊的能力
而BERT做的事情就像是De-noising Auto-encoder
Feature Disentangle Auto-encoder在Encoder中會將複雜的高維向量變成簡單的code再透過Decoder變回複雜的高維向量，而這些code內有重要的資訊，但哪些維度代表什麼資訊並不知道
因此有了Feature Disentangle的議題，在訓練Auto-encoder時Encoder輸出的向量，哪些維度代表哪些資訊
若想了解Feature Disentangle是如何實現可以參考以下論文 :
 One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization : Link Multi-target Voice Conversion without Parallel Data by Adversarially Learning Disentangled Audio Representations : Link AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss : Link  Application: Voice Conversion 在過去要做語者轉換需要兩種不同音色的人講相同的話來進行Supervised learning，但這非常不實際。使用Feature Disentangle的技術，可以做到透過訓練不同的話來進行語者轉換</description>
    </item>
    
    <item>
      <title>ORBSLAM2 程式碼解析 - Frame</title>
      <link>https://Offliners.github.io/post/orbslam2-code-4/</link>
      <pubDate>Wed, 28 Apr 2021 20:50:07 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/orbslam2-code-4/</guid>
      <description>System Overview Source Code : Link Frame是指幀，也就是對應的圖像，可以是一元、二元或RGBD，所以此類別所包含SLAM中以幀為單位的操作，有以下方面 :
 讀寫該幀對應的相機位姿 處理幀和特徵點之間的關係，包括判斷特徵點是否在視野內、獲取該幀依定區域內的特徵點、校正特徵點等 恢復深度，如果是RGBD就直接讀取深度值，如果是一元或二元則進行深度恢復  以下為Frame類別中的主要函數 :
Frame │ ├── Frame // 建構函數，有以下五種 ├────── 預設建構函數 ├────── 複製建構函數 ├────── 二元幀建構函數 ├────── RGBD幀建構函數 ├────── 一元幀建構函數 │ ├── ExtractORB // 提取ORB特徵點 │ ├── ComputeBoW // 計算詞袋數據 │ ├── SetPose // 設置相機位姿參數 │ ├── isInFrustum // 判斷路標點是否在視野中 │ ├── GetFeaturesInArea // 查找特定區域內的特徵點 │ ├── ComputeStereoMatches // 利用二元恢復深度 │ ├── ComputeStereoFromRGBD // RGBD獲取深度訊息 │ ├── UnprojectStereo // 特徵點座標反投影到3D地圖點 建構函數 建構函數除了預設與複製之外，其他主要對應三種相機模型，主要功能也類似，就是提取特徵點，然後把特徵點劃分到網格中，使特徵點在圖像中分布較均勻</description>
    </item>
    
    <item>
      <title>ORBSLAM2 程式碼解析 - MapPoint</title>
      <link>https://Offliners.github.io/post/orbslam2-code-3/</link>
      <pubDate>Wed, 28 Apr 2021 14:04:05 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/orbslam2-code-3/</guid>
      <description>System Overview Source Code : Link MapPoint是地圖中的特徵點，自身參數是3D座標與描述子，在此類別中會完成以下工作 :
 維護關鍵幀之間的共視關係 通過計算描述向量之間的距離，在多個關鍵幀的特徵點中找到最匹配的特徵點 在閉環完成修正後，需要根據修正的主幀位姿來修正特徵點 對於非關鍵幀，也能產生MapPoint，只是是用來給Tracking功能臨時使用  以下為MapPoint中的主要函數
MapPoint │ ├── MapPoint // 建構函數 ├──────── 關鍵幀地圖點建構函數 ├──────── 普通幀地圖點建構函數 │ ├── AddObservation // 增加地圖點觀測關係 │ ├── EraseObservation // 刪除地圖點觀測關係 │ ├── SetBadFlag // 刪除地圖點 │ ├── Replace // 替換地圖點 │ ├── ComputeDistinctiveDescriptors // 計算描述子 │ ├── UpdateNormalAndDepth // 更新法向量和深度值 │ ├── PredictScale // 預測尺度 建構函數 MapPoint 一共有兩個，分別對應關鍵幀與普通幀
關鍵幀的地圖點建構函數 MapPoint(const cv::Mat &amp;amp;Pos, KeyFrame *pRefKF, Map* pMap) Pos為該點的3D位置，pRefKF是參考關鍵幀，pMap是地圖。關鍵幀的地圖點建構函數主要是突出地圖點與關鍵幀之間的關係，一個地圖點會被多個關鍵幀偵測到，多個關鍵幀之間透過共同偵測到的地圖點來產生關係，叫做共視關係。在ORBSLAM中，是透過MapPoint來維護共視關係。在進行BA優化時，指優化具有共視關係的關鍵幀，其他關鍵幀的位姿則不參與優化</description>
    </item>
    
    <item>
      <title>ORBSLAM2 程式碼解析 - System</title>
      <link>https://Offliners.github.io/post/orbslam2-code-2/</link>
      <pubDate>Tue, 27 Apr 2021 19:39:14 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/orbslam2-code-2/</guid>
      <description>System Overview Source Code : Link 系統流程的入口在System.cc，裡面有4個主要的函數 :
 System TrackStereo TrackRGBD TrackMonocular  其中System是SLAM系統的建構函數，包括所有功能模塊與所有線程的初始化。而TrackStereo、TrackRGBD、TrackMonocular分別為二元、深度與一元相機的數據入口，使用ORBSLAM時會先透過System來生成SLAM系統的使用對象，然後根據傳感器的類型來選擇相對應的入口來傳入數據
建構函數 System System // 建構函數 │ ├── mpVocabulary = new ORBVocabulary(); // 加載ORB詞袋模型 │ │── mpKeyFrameDatabase = new KeyFrameDatabase(*mpVocabulary); │ // 創建關鍵幀數據集，數據集中主要存放詞袋模型中的值，這些值用於閉環檢測與重定位 │ │── mpMap = new Map(); // 創建地圖 │ │── mpFrameDrawer = new FrameDrawer(mpMap); // 畫關鍵幀的Drawer │ │── mpMapDrawer = new MapDrawer(mpMap, strSettingsFile); // 畫地圖的Drawer │ │── mpTracker = new Tracking(this, mpVocabulary, mpFrameDrawer, mpMapDrawer, mpMap, mpKeyFrameDatabase, strSettingsFile, mSensor); │ // 建立跟踪線程 │ │── mpLocalMapper = new LocalMapping(mpMap, mSensor==MONOCULAR); │ // 建立局部地圖線程 │ │── mpLoopCloser = new LoopClosing(mpMap, mpKeyFrameDatabase, mpVocabulary, mSensor!</description>
    </item>
    
    <item>
      <title>Legends of Code &amp; Magic 新手指南</title>
      <link>https://Offliners.github.io/post/locm/</link>
      <pubDate>Sat, 24 Apr 2021 11:42:41 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/locm/</guid>
      <description>任務簡介  這是一個類似爐石戰記，玩家對玩家的卡牌遊戲 必須組出屬於你自己的牌組(deck)，並且與對手組好的牌組進行對戰 雙方玩家都有 30 點的血量，誰先把對方的血量降至 0 或更低就獲勝了  遊戲規則  玩家數 : 2人 (你與電腦)  選牌階段 每位玩家必須組出 30 張牌的牌組，每一回合都會提供你 3 張牌來挑選，你可以選擇其中一張加你的牌組，重複這個動作 30 次，你就成功的組完你的牌組
組牌階段 戰鬥回合制 - 初始規則  遊戲分為先攻方以及後攻方，開始時，先攻方抽 4 張牌，後攻方抽 5 張牌，且在每一個回合，每一個玩家都可以從牌組抽出一張牌，有些牌可能擁有讓你下回合能夠多抽牌的能力 先攻方剛開始只會有 1 顆水晶，後攻方有 2 顆水晶。直到後攻方將其水晶一回合性的花光後，後攻方多出來的 1 顆水晶才會消失 每一回合每個玩家的水晶最大上限都會成長一顆，且回復至最大上限的水晶，最多成長到 12 顆水晶(如果後攻方一直沒有在某回合花完他全部的水晶，則可以達到 13 顆的上限) 玩家可以想辦法將自己的水晶花完來打出各種牌  出牌階段 戰鬥回合制 - 出牌以及生物規則   玩家可以從手牌召喚生物(SUMMONING)到場上，並付出同等於該生物的水晶花費，這些生物可以用來攻擊對手的生物或玩家，或者是防禦對手的生物
  有些生物可能含有特殊能力
  通常來說，生物剛上場的回合不能攻擊(除非此生物含有C的能力)，且一回合只能攻擊一次
  當生物互相攻擊時，雙方的攻擊力會從彼此的防禦力互相扣除，若直接攻擊玩家，則該玩家的生命值扣除該生物的攻擊力   當某隻生物的防禦力變成 0 或更低，那隻生物就會被從場上移除</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 8 - BERT</title>
      <link>https://Offliners.github.io/post/ntuml-week8/</link>
      <pubDate>Wed, 21 Apr 2021 21:33:28 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week8/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2 Video 3 Video 4
Youtube Video Link (English): Video 1 Video 2 Video 3 Video 4
Model Introduction Self-Supervised Learning的模型命名皆來自芝麻街的角色，剩下Cookie Monster(藍)還尚有模型以此命名
說到Self-Supervised Learning就要提一下2018年出現的BERT模型，他是個超巨型的Transformer，有340 Million個參數，是一般Transformer參數量的3000多倍
直到現今還有更多參數量更大的模型出現
   Model Parameter     ELMO 94M   BERT 340M   GPT-2 1542M   Megatron 8B   T5 11B   Turing NLG 17B   GPT-2 175B   Switch Transformer 1.</description>
    </item>
    
    <item>
      <title>ORBSLAM2 程式碼解析 - 論文閱讀</title>
      <link>https://Offliners.github.io/post/orbslam2-code-1/</link>
      <pubDate>Sat, 17 Apr 2021 00:46:46 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/orbslam2-code-1/</guid>
      <description>ORB-SLAM2: an Open-Source SLAM System for Monocular, Stereo and RGB-D Cameras (https://arxiv.org/abs/1610.06475) ORBSLAM2 : https://github.com/raulmur/ORB_SLAM2 ORBSLAM2 for Windows : https://github.com/phdsky/ORBSLAM24Windows  System Overview 變數命名規則    開頭 變數資料型態     n int   b bool   p pointer   s set   v vector   l list   m class member variable    Coming Soon</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 7 - GAN</title>
      <link>https://Offliners.github.io/post/ntuml-week7-2/</link>
      <pubDate>Thu, 15 Apr 2021 20:04:48 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week7-2/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2 Video 3 Video 4
Youtube Video Link (English): Video 1 Video 2 Video 3 Video 4
Network as Generator 現在要將network當成Generator使用，這個network會輸入x與隨機變數z來生成y，每次使用這個network時會透過簡單的分布來生成不同的z，這種簡單的分布可以是Gaussian Distribution或者是Uniform Distribution等。加入隨機變數z後，network輸出會變成複雜的分布。這種特殊的network叫做Generator
那為什麼要訓練Generator?為什麼要訓練這種輸出是一個複雜分布的模型?
Why Distribution? 假設今天有個任務，是要去預測之後的遊戲畫面，那可能會將過去遊戲畫面作為模型輸入，然後模型會輸出下一個預測的遊戲畫面
但這樣會有個問題，就是有時候會發現小精靈會分裂，甚至消失。因為在訓練資料中有時候小精靈是向左轉，有時候是向右轉，那會造成機器覺得向左轉是對的，向右轉也是對的，但同時向左向右就是錯的，那要怎麼處理這類問題呢?讓機器輸出是有機率的，而不是單一的輸出
將給network一個機率分布z時，模型輸出就會變成一個Distribution，這個Distribution包含向左轉與向右轉的可能
那什麼時候需要這種network?當任務需要一些創造力的時候，也就是同樣的輸入卻有多種可能的輸出，而這些輸出都是對的。例如繪畫或者聊天機器人
Generative Adversarial Network GAN有多種的變形，多到許多不同GAN的變形卻有相同的名字，可以到以下連結去查看:
 The GAN Zoo : Link  Anime Face Generation 接著要舉的例子是生成動畫人物的臉，使用的是Unconditional Generation，也就是模型輸入只有Z，沒有x。因此Generator只輸入Z輸出y，然後Z是使用Normal distribution中sample出來的向量，向量維度是自訂的。所以透過輸入低維度的隨機向量，去透過Generator產生圖片這種高維度的向量(如果圖片大小是64 * 64且是RGB，那輸出的向量就會是64 * 64 * 3)
Discriminator 在GAN中，除了訓練Generator，還要訓練Discriminator，圖片會輸入到Discriminator，接著輸出一個數值，數值越大表示這張圖片越像是二次元人物的圖像
Basic Idea of GAN 首先第一代的Generator因為參數是隨機的，因此產生的圖片就是雜訊，那換Discriminator來分辨這些圖片與真正的圖片不同，一開始Discriminator可能只判斷是否有眼睛，有眼睛是真正的圖片，沒有的是Generator產生的。那換第二代的Generator，調整參數來騙過Discriminator，因為Discriminator判斷的依據是是否有眼睛，所以第二代的Generator開始產生眼睛，可以騙過第一代的Discriminator，但Discriminator也會進化，第二代的Discriminator可能發現可以根據是否有頭髮或嘴巴來分辨圖片。那第三代的Generator就會繼續想辦法去騙過第二代的Discriminator。以此類推，所以Generator產生的圖片會越像二次元人物，而Discriminator也會越來越嚴苛</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 7 - Transformer</title>
      <link>https://Offliners.github.io/post/ntuml-week7-1/</link>
      <pubDate>Tue, 13 Apr 2021 20:43:16 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week7-1/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2
Youtube Video Link (English): Video 1 Video 2
Applications of Sequence-to-sequence (Seq2seq) 當模型輸入是sequence時，輸出可以是與輸入相同長度的sequence，也可能是未知長度的sequence，由機器去決定。輸出是未知長度sequence的例子如
 語音辨識 Speech Recognition 機器翻譯 Machine Translation 語音翻譯 Speech Translation  這時可能有人問為何要做語音翻譯?直接語音辨識再機器翻譯不就好了?因為世界上有眾多語言，有些語言甚至連文字都沒有，沒有文字的語言難以做語音辨識
若想了解台語語音辨識可點以下連結: To learn more : Link
輸入聲音，輸出辨識後的文字是語音辨識 Speech Recognition。反過來，輸入文字，輸出聲音訊號，就是語音合成 Text-to-Speech Synthesis
Seq2seq for Chatbot 文字方面，也可透過使用Seq2seq的模型來訓練，例如可以訓練聊天機器人，輸入輸出都是文字，訓練資料是一堆對話的內容
Most Natural Language Processing applications 很多NLP的問題可以看成是QA的問題，多數QA的問題又可以透過Seq2seq解決。輸入問題與文章，輸出就是問題的答案
想了解更多NLP的處理，可參考以下連結 To learn more :
 The Natural Language Decathlon: Multitask Learning as Question Answering : Link LAMOL: LAnguage MOdeling for Lifelong Language Learning : Link DEEP LEARNING FOR HUMAN LANGUAGE PROCESSING 2020 SPRING : Link  Seq2seq for Syntactic Parsing Seq2seq也可用於文法剖析，輸入是一段文字，但輸出就不是sequence，而是樹狀結構用來分析各文字的組成</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 5 - Normalization</title>
      <link>https://Offliners.github.io/post/ntuml-week5-2/</link>
      <pubDate>Sun, 11 Apr 2021 15:34:25 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week5-2/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video
Youtube Video Link (English): Video
Changing Landscape 如果有個簡易的模型，x1輸入值小，因此當w1改變時，loss也會有微小的改變，但x2因為輸入大，所以對loss的改變也會很大，這使error surface變成橢圓形。因為兩參數對loss斜率差很大，導致error surface相當難訓練，因此要使用Batch Normalization來使error surface變比較好訓練
Feature Normalization 透過這種Normalization，最後得到平均為0，且變異數是1，因此數值分布會在0左右，所以做出比較好得error surface，做梯度下降時，能收斂得快一些
將Normalization後得數值丟入network做計算，進一步思考會發現只有W1輸入的值有Normalize，但W2沒有，那如果z得數值差異大那對W2的訓練會不會變困難?因此輸入W2的值也應該要做Normalize
Q : 那要在activation function前做Normalize還是之後做?
A : 實作上其實差異不大，如果activation function是使用sigmoid的話建議在之前做
假設現在對z做Normalize
由於z、mean與std都是向量，因此這裡的除法是對向量中的每個對應的元素計算。由於mean與std是經過z1、z2與z3計算得到的，沒做Feature Normalization時，各個是獨立計算的，z1只影響a1，但當Feature Normalization後，z1改變時，會對影響到a2與a3。因此現在的network不是分別處理，而是一次處理多個輸入。由於GPU記憶體有限，因此network不會一次考慮整個訓練資料，而是考慮一個batch，這就是Batch Normalization，但這適用於batch size較大的情況，這樣計算batch的mean與std比較能得到資料的分布
有時Batch Normalization還加入兩個未知參數來做訓練，這兩參數需要透過模型另外再學習出來，那為何需要加入這兩未知參數呢?由於做完Batch Normalization時，平均值為0對network多了限制，因此透過這兩參數來調整
有Batch Normalization後要怎麼測試呢?一般會想說一次測試一組batch的資料，但實際上不會等訓練出一組batch再測試，如pytorch會使用moving average來將訓練的mean與std進行處理，測試時就不用再計算batch的mean與std，而是使用moving average後的數值來測試
上圖是Batch Normalization論文中的圖表，可以發現有做Batch Normalization的模型可以使用較短的時間達到終點
原始Batch Normalization提出Internal Covariate Shift來解釋為何Batch Normalization能優化，簡單來說是更新參數時，更新後的參數是對更新前的參數比較好不是對更新後的，因此做Batch Normalization能先使原始參數的分布較接近更新後的，使更新參數的方向往適合更新後參數的方向走，但仍然有其他論文做實驗來解釋其他理由
上圖所描述文章講解透過實驗與理論來證實Batch Normalization能改變error surface，使error surface變得比較不崎嶇。要改變error surface其實還有其他方法，試其他方法改變error surface後發現與Batch Normalization的表現差不多，因此感嘆說Batch Normalization是偶然發現的，但是是一種有用的方法
To Learn More  Batch Renormalization : Link Layer Normalization : Link Instance Normalization : Link Group Normalization : Link Weight Normalization : Link Spectrum Normalization : Link  </description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 5 - Self-Attention</title>
      <link>https://Offliners.github.io/post/ntuml-week5-1/</link>
      <pubDate>Sat, 10 Apr 2021 23:06:41 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week5-1/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Vedio 2
Youtube Video Link (English): Video 1 Vedio 2
Sophisticated Input 以往的輸入(預測Youtube觀看人數、影像處理等)都可以看成是一個向量，那如果輸入變成一排向量呢? 或者輸入許多大小不一的向量呢?
那有什麼輸入是Sequence且長度會改變的呢?如文字處理，輸入是多個句子，且句子長度不同。那要怎麼把詞彙變成像量呢?最簡單的做法是one-hot encoding，假如有個很長的向量，向量長度為世界上存在詞彙的數目，當出現某個詞彙，那該詞彙所在的index就會為1。除了需要非常長的向量之外會存在一個嚴重的問題，那就是詞彙之間沒有關係，即使詞彙都是動物的種類，但彼此沒有關係，因為這種表示法沒有語意的資訊。因此有word emdeding，想得知更多可觀看以下連結的影片
 To Learn More : Unsupervised Learning - Word Embedding Video  還有聲音訊號也可當成長度不一的向量組，可以把聲音訊號取一個範圍，將這裡面的資訊描述成向量，也就是一個Frame，將一段訊號描述成向量有多種作法。為了描述整個整個向量會移動這個範圍，再將這裡面描述成新的向量，因此就能組成向量組
還有Graph也能看成是向量組，每個節點可以描述成向量，這些向量也能組成向量組來描述Graph
既然Graph能描述成向量組，那分子也可以，每個分子當成一個Graph，而每種元素能用one-hot encoding來描述成向量
What is the output? 既然輸入是向量，那輸出是麼呢?首先輸入與輸出的向量可以對應到一個Label，如果Label對應到數值，那就是Regression問題;如果對應到class，那就是Classification問題，這種形式輸入輸出的長度是相同的。有以下應用:
 POS Tagging 詞性標註 語音處理 Social Network Porblem  那還有輸入向量組，輸出只是一個Label。有以下應用:
 Sentiment Analysis 情緒分析 語者辨認 分子分析  最後一種可能的輸出是不知道要輸出多少Label，機器要自己決定，這種任務叫做Sequence to Sequence</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 3 - CNN</title>
      <link>https://Offliners.github.io/post/ntuml-week3-3/</link>
      <pubDate>Thu, 08 Apr 2021 21:12:19 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week3-3/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video
Youtube Video Link (English): Video
Image Classification 通常會假設輸入圖片大小是固定的，如果圖片大小不一樣，會先Rescale。
Neuron Version Story about Convolutional Layer 通常一張彩色圖片會是3維張量，分別為圖片的長、寬與Channel，這三個Channel為圖片的RGB。接著會把tensor給拉直，變成一維向量，那要如何拉直呢?將每個channel的數值排一排，就可以當成network的輸入，這個向量的每個數值，代表該顏色的強度。
將這個向量輸入到fully-connected network，如果輸入有100 * 100 * 3個feature，經過第一層假設有1000個神經元，那這一層會有30000000個weight。那這麼多參數會有什麼問題?雖然參數增加可以增加模型的彈性與能力，但也增加了訓練overfit的機率。考慮到影像的自身特性，其實不需要每個feature都有一個weight。那影像有什麼特性呢?
Observation 1 對於影像辨識而言，希望神經元內能辨識的是圖片部分的pattern。比如圖片出現鳥嘴、鳥眼與鳥腳，因此能辨識出這是隻鳥
因此神經元不需要逐一對應到每一個特徵，神經元只需要對應到小區域的特徵群就好
在CNN中，會定義一個區域叫Receptive field，這範圍內的特徵會被拉直輸入到一個神經元中，因此神經元只需要輸入3 * 3 * 3個特徵，相較之前的100 * 100 * 3少了許多
那要怎麼決定Receptive field呢?這要自行決定。且Receptive field可以彼此重疊，甚至多個神經元去守備相同的Receptive field。
那Receptive field可以有不同大小嗎? 可以
那Receptive field可以有只考慮一個channel嗎? 可以
那Receptive field可以用長方形嗎? 可以
雖然Receptive field可以自行設定，那這邊要介紹經典的安排方式
會涵蓋所有的channel，因為深度會全涵蓋，所以只需要講高和寬，這個高和寬是kernel size，通常不會太大，常見的是(3, 3)
Receptive field會由一組多個神經元去守備，不會只有一個
Receptive field會移動到新的Receptive field，移動的長度是stride，通常不會太大(通常設1或2)，這樣才能重疊。假設Receptive field完全沒有重疊的話，如果有個pattern出現在兩個Receptive field的重疊處時，就會沒有神經元去偵測，就會遺失這個pattern。因此會希望Receptive field高度重疊</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 3 - Loss Function: Classification</title>
      <link>https://Offliners.github.io/post/ntuml-week3-2/</link>
      <pubDate>Tue, 06 Apr 2021 15:46:03 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week3-2/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video
Youtube Video Link (English): Video
Classification as Regression? 如果把Classification當成Regression來看，將每個class用編號來記錄，那意味著class 1跟class 2比較像，class 1跟class 3比較不像，因此這方法會有點瑕疵
透過使用one hot vector，就不會有上述問題，因為把one hot vector算距離，class之間兩兩距離都是相同的。那現在目標需要三個數值，但以前regression的output都是一個數值，那要怎麼做呢?
那只需要將原本output一個數值的方法重複三次，乘上3個不同的weight加上bias得到三個數值，得到的這個向量期望與目標向量越接近越好
在做Classification時，往往會加上輸出向量再通過softmax得到另一個向量再去跟label做比較。簡單來說，因為label是0或1的值，因此通過softmax能將任何值移到0~1之間，方便來做跟label的比較
Softmax 透過上次可得知，每個y值介於0~1之間，且總和為1。這裡討論三個class的情形，那如果只有兩個class呢?也可以使用softmax，但常聽到的是sigmoid，其實兩者是等價的
Loss of Classification 以前計算預測值與實際值的距離可能常用MSE，但在分類問題更常用cross-entrory。使用pytorch時，cross-entrory與softmax是綁在一起的，使用cross-entrory時，pytorch會自動將softmax加到最後一層
 Mathematical proof : Video  比較兩者的error surface，如果y1大y2小時loss小，y1小y2大時loss大，因此期望參數能走到右下角loss小的地方。假設一開始在左上角，由於在cross-entrory下左上角仍有斜率，因此能往右下走。但在MSE下，左上角非常平坦，gradient小，使用梯度下降法會卡住，有很大機率會訓練不起來，但使用Adam會調大learning rate，還是有機會走到右下角，只是訓練還是困難的。總結來說，改變loss function能改變訓練的難易度
To Learn More  Classification : Video Logistic Regression : Video  </description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 3 - Tips for Training: Adaptive Learning Rate</title>
      <link>https://Offliners.github.io/post/ntuml-week3-1/</link>
      <pubDate>Tue, 06 Apr 2021 14:24:24 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week3-1/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video
Youtube Video Link (English): Video
Training Stuck equals Small Gradient? 在訓練network時，通常會記錄loss，會發現loss越來越小，然後就卡住了不再下降，通常會覺得是遇到critical point，但真的是如此嗎?多數人可能不會確認說此時的gradient真的是否很小。上圖例子可以發現，loss不再下降，但gradient沒有很小。
假設今天有個error surface，在參數b的變化非常小，參數w非常陡峭。如果learning rate設0.01時，會在山壁上震盪，無法滑進山谷。但learning rate設0.0000001時，雖然進入山谷，但仍然無法到終點，因為learning rate太小，步伐過小使參數要更新過多次也無法踏出多大的距離
會有這樣的問題在於learning rate是固定的，因此learning rate需要為每個參數客製化
Different parameters needs different learning rate 大原則 : 在某個方向上，gradient很小非常平坦，會希望有較大的learning rate;gradient很大非常陡峭，會希望有較小的learning rate
為簡單起見，只用一個參數來講解。
將原本的learning rate除以一個參數，這個參數是depend on i，為每個參數客製化，接下來看說這個參數有什麼常見的計算方式
Adagrad 首先是，過去所有gradient的均方根
這被使用在Adagrad的方法中
那這會有什麼問題呢?由於每個參數會需要的learning rate可能會隨時間改變，因此會希望同參數同方向，也能有動態調整的learning rate
RMSProp 透過alpha來調整現在與過去的gradient的重要性，這個alpha也是一個超參數要自行設定。如果alpha設接近0，表示現在的gradient比較重要。反之設接近1，表示過去的gradient比較重要
透過此機制，可以視gradient來動態調整learning rate
Adam 最常使用的，Adam是RMSProp結合Momentum
Training with Adagrad 之前沒有使用Adagrad來訓練，update參數10萬次只左一小步。但有了Adagrad來訓練，update參數10萬次較到達接近終點的地方，因為在平緩的地方會自動條大learning rate。那為什麼在終點時爆炸了呢?由於Adagrad是累積過去的gradient，因此在縱軸上每次累積很小的gradient，到一定程度時爆走，但沒關係，當衝出去時到大的gradient，會使sigma變大那update參數的步伐又會變小又回到峽谷，那如何解決這問題呢?
隨著時間的增加，距離終點也越近，讓learning rate越來越小，使參數更新能慢下來，因此能解決剛剛的狀況
除了decay的方法，還有warm-up，讓learning rate先變大後變小，變大變小的速率也是一種超參數。
warm-up在訓練BERT常用到，此外在早期模型也常用到，如要有更多理解可以參考RAdam(https://arxiv.org/abs/1908.03265)這篇論文
Summary of Optimization 有可能會有人問momentum與sigma都是考慮過去所有的gradient，那一個在分子一個在分母不會抵銷嗎?</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 2 - Tips for Training: Batch and Momentum</title>
      <link>https://Offliners.github.io/post/ntuml-week2-3/</link>
      <pubDate>Mon, 05 Apr 2021 21:55:10 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week2-3/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video
Youtube Video Link (English): Video
Optimization with Batch 每次update參數是將每個Batch出來計算loss與gradient，將每個batch計算過後是一個epoch。每個epoch都會經過shuffle，因此每個epoch中batch的資料都不一樣
Small Batch v.s. Large Batch Large Batch 須看完每筆資料才更新參數，因此更新時間長，但每次更新很平穩
Small Batch 每看完一筆資料就更新參數，因此更新時間短，但每次更新會顯得不穩
但考慮平行運算的話，Large Batch時間不一定比Small Batch長
由上圖可知batch從1~1000所花時間差不多，但GPU仍有極限，過大的batch size仍會需要花數倍的時間
如果batch size等於1，那60000筆資料需要update參數60000次。batch size等於1000，那60000筆資料需要update參數60次。兩個每次update的時間差不多，但跑完一個epoch的差距就很大。因此考慮平行運算後，大batch size反而比較有效率
考慮平行運算後，大batch沒有時間劣勢了，且update參數比較穩定，那大batch比較好嗎?小batch比較差?
雖然在小batch size下update參數比較不穩，但有noisy的gradient反而能幫助訓練。比較上面兩資料集，小batch size的準確率比較高。那這是什麼問題呢?這是Optimization Issue
那為何小batch size沒有Optimization Issue呢?如果使用full batch size的話，走到local minima時就會停止更新參數。但如果使用small batch size的話，可能其中一個batch卡住了，但其他batch可能沒有。因此這種noisy的update參數反而對訓練比較有幫助
訓練上small batch size比較好，那測試呢?透過以上的實驗，將大batch和小batch訓練到差不多好，那比對測試時，發現小batch在測試集上表現比較好。大batch在訓練好，測試差，表示發生了overfitting。那為何會這樣呢?
假設有一個loss，有許多local minima，但local minima有分好壞，那怎麼區分好壞呢?如果local minima落在峽谷中(上圖右側)，會認為是壞minima;反之，落在平原中會是好的minima。會這樣區分是因為，測試與訓練的loss或有些差別，平原型的minima差異較小，峽谷型的minima會有較大的loss差異。而小batch因為update參數的方向都不同，容易跳出峽谷的loss，容易落在平原型的minima;對大batch來說，反而容易落在峽谷的minima
那有沒有辦法堅固兩者的優點呢?以下的論文有探討
 Large Batch Optimization for Deep Learning: Training BERT in 76 minutes (https://arxiv.</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 2 - Critical Point: small gradient</title>
      <link>https://Offliners.github.io/post/ntuml-week2-2/</link>
      <pubDate>Mon, 05 Apr 2021 00:38:19 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week2-2/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video
Youtube Video Link (English): Video
Why Optimization fails? 有時會發現隨著參數不斷update，training的loss不再下降(藍線)，但對這loss仍不滿意。跟其他如linear model以較發現deep network沒有更好，顯然optimization有問題。有時還會發現一開始model就train不起來，不管怎麼update參數loss仍不降(紅線)。那這是發生什麼事呢?
過去的猜想是，由於模型訓練到一個地方，這地方參數對loss的微分為0，梯度下降法就沒辦法再update參數，所以loss不再下降。
說到Gradient為0，有兩種常見的可能:
 Local Minima 區域最小值 Saddle Point 鞍點 Local Maxima 區域最大值  這些會使Gradient為0的點統稱Critical point，那如果今天Gradient很接近0，那是卡在哪一個呢?等等會解說如何判斷
那又為何需要分辨卡在哪裡?如果卡在local minima那就沒路可走，但如果是saddle point那旁邊還有路可以走，可以讓loss更低。因為update參數是往低處走，所以不太會卡在local maxima
Math Method 要判斷是local minima還是saddle point呢?首先要知道loss function的形狀，由於模型通常過於複雜，使得我們無法得知loss function的全貌，但可以給定某一組參數來分析在這組參數下loss function附近的樣貌
Tayler Series Approximation 透過泰勒級數近似法可以將loss function在這組參數下分成以上三個的組合
如果今天卡在critical point，表示gradient為0，那綠色那項也為0，因此可以根據紅色那項判斷這組參數附近的error surface是長什麼樣子，進而判斷卡在哪一種critical point
為方便表示，將 $$ v = (\theta - \theta^{&#39;})$$
  紅色項帶任何值皆大於0，表示這組參數是附近的最低點，因此是local minima
  紅色項帶任何值皆小於0，表示這組參數是附近的最高點，因此是local maxima</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 2 - Guideline of ML: overfit</title>
      <link>https://Offliners.github.io/post/ntuml-week2-1/</link>
      <pubDate>Sat, 03 Apr 2021 23:32:44 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week2-1/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video
Youtube Video Link (English): Video
Framework of ML General Guide 如果覺得訓練不滿意時，首先檢查training data的loss，看模型是否學起來再檢查testing data
有可是model bias使training data的loss無法變低。由於model過於簡單，透過訓練所有參數所得到的function set，但因為模型簡單使得function set小，因此這個function set沒有包含任何一個function使得loss變低，可以讓loss變低的function不在這個model可以描述的範圍裡面。簡單來說，想在大海裡撈針(使loss變低的function)，但針根本不在海裡，怎麼撈都撈不到。
解決方法 : 重新設計模型，使模型有更大的彈性，可以增加feature，也可以使用deep learning的方式
但不代表training時loss大就一定是model bias，有可能是optimization做不夠好
這堂課都使用梯度下降法，但這方法有些問題，如可能卡在區域最小值。如上圖藍色圖形內表示model可以表示函式的集合，這集合內確實存在loss低的function，但梯度下降法沒辦法幫我們找到他。簡單來說，想大海撈針，針確實在海裡，但我們沒辦法撈起來。
當training data的loss不夠低時，是哪個問題呢?
可以透過比較不同的模型來判斷。如果看testing data的loss表現，很多人可能覺得56層沒有比20層做得好，這是overfitting。但這不是overfitting，透過比較training data的loss，發現20層的loss比56層的低，代表56層的optimization沒有做好。這也不會是model bias的問題，56層彈性更大，一定能做到20層能做到的事，因此這就是Optimization Issue。
因此要知道optimization有沒有做好，可以先跑一些小的或是淺的模型，甚至用一些不是deep learning的方法，如Linear model或是SVM，他們比較容易做optimization。接著做深的model，如果深的model彈性大但loss沒辦法做得比淺的低，表示有Optimization Issue。以上次例子來看，模型5層出現了Optimization Issue。那要怎麼辦呢?下堂課會講解
如果已經可以讓training data的loss變小，那就可以看testing data的loss，當testing data的loss也小，那就訓練完成。
那當覺得testing data的loss還不夠小時，如果training data的loss小，testing data的loss大，那有可能遇到overfitting的問題。
舉一個極端例子，假設今天有個模型，如果輸入有出現在training data中那就輸出他的值，若沒有則輸出隨機值。此模型在training data的loss為0，但在沒看過的teating data的loss變得很大
一般情況下，假設今天實際資料呈現為二次曲線，但我們不知道，我們只能知道線上的3個點(訓練資料)，如果有個能力強的模型，彈性很大，他會通過訓練資料的三個點，但其他地方會呈現freestyle，因此丟入測試資料時會發現沒通過使loss變大
那要如何解決overfitting呢?方法一是增加訓練資料，也是最有效解決的方法。方法二是Data augmentation，如做影像辨識時，可以翻轉資料來增加資料，這Data augmentation不能隨便做，如翻轉圖片不會顛倒圖片，因為這可能不是真實世界會出現的影像。因此要根據對資料的理解，來選擇合適Data augmentation的方式
剛剛是增加資料的方式，也可以透過增加模型限制來避免overfitting。比如可以限制模型一定是二次曲線，但要如何得知呢?取決於對問題的理解。最好的是模型與資料背後產生得過程相同
如何限制模型呢?
 漸少參數、神經元，或者共用參數(CNN) 用比較少的feature Early stopping Regularization Dropout  但也不要給模型過多的限制。假設模型限制是一條直線，雖然沒辦法找到一條同時通過這三個點，但可以找到一條線loss是最低的，但這會在testing data中得到較大的loss。那這是overfitting嗎?</description>
    </item>
    
    <item>
      <title>NTU Machine Learning Week 1 - Introduction of ML/DL</title>
      <link>https://Offliners.github.io/post/ntuml-week1/</link>
      <pubDate>Sat, 03 Apr 2021 17:05:34 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/ntuml-week1/</guid>
      <description>NTUML 2021 Spring Course Syllabus: Link
Youtube Video Link (Chinese): Video 1 Video 2
Youtube Video Link (English): Video 1 Video 2
What is Machine Learing? Q : 什麼是機器學習?
A : 機器學習簡單來說就是讓機器學會找一個函式的能力 舉例來說:
 語音辨識(Speech Recognition) : 函式輸入是聲音訊號，輸出是這段訊號的內容 影像辨識(Image Recognition) : 函式輸入是圖片，輸出是這張圖片的內容 Alpha Go : 函式輸入是棋盤上黑白棋的位置，輸出是機器下一步落子的位置  Different types of Functions  Regression : 函是輸出為純量 舉例 : 輸入今天的PM2.5濃度、溫度等，預測明天的PM2.5濃度 Classification : 給定選項，函示輸出為機器從這些選項中的選擇 舉例 : 輸入一封電子郵件，輸出為是否為垃圾郵件  Alpha Go也是一種Classification，輸入棋盤上黑白棋的位置，輸出棋盤19 * 19可以落子的位置選出下一步應該落子的位置
Q : 機器學習只有這兩個任務嗎?</description>
    </item>
    
    <item>
      <title>ORBSLAM2 Build on Windows 10</title>
      <link>https://Offliners.github.io/post/orbslam2-build/</link>
      <pubDate>Thu, 01 Apr 2021 01:31:05 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/orbslam2-build/</guid>
      <description>測試環境    Name Version     Windows 10 x64   OpenCV 3.4.1   Visual Studio 2015   CMake 3.20     OpenCV : Install Link Visual Studio 2015 : Install Link CMake : Install Link  記得將YOUR_OWN_PATH\opencv\build\x64\vc14\bin與YOUR_OWN_PATH\opencv\build新增至環境變數中，資料夾路徑視OpenCV的安裝目錄而定
安裝ORBSLAM2 for Windows 下載 ORBSLAM2 Download : Link
安裝DBoW2 使用Cmake-gui將Source code路徑選.\orbslam-windows\Thirdparty\DBoW2，而binaries選擇.\orbslam-windows\Thirdparty\DBoW2\build，若沒有build資料夾程式可以自動建立
接著按Configure選擇Visual Studio 14 2015，平台選擇x64或是你的作業系統而定
按下finish開始編譯，Configuring done後選擇Generate，等Generating done後按Open Project開啟VS
進入Visual Studio 2015後，先將Mode選擇Release，接著對DBoW2按右鍵選擇Properties，將Target Extension的.dll改成.lib，還有Configuration Type改成Static library(.</description>
    </item>
    
    <item>
      <title>[110學年度] 台清交推甄心得</title>
      <link>https://Offliners.github.io/post/graduate-school/</link>
      <pubDate>Tue, 30 Mar 2021 22:23:42 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/graduate-school/</guid>
      <description>個人基本資料  學校 : 國立臺灣師範大學  主修 : 機電工程學系 雙主修 : 電機工程學系   在校成績 : 系排 2 (4%) 平均 GPA : 4.15 (學分數: 142) 社團經歷  第二屆機電營 美輔股 股員 第三屆機電營 場器股 股長 第四屆機電營 美輔股 培訓講師 第四屆機電營 場器股&amp;amp;課程股電路組 股長 機電工程系學會 家長股長 全校英語讀書會成果發表 第三名   教學助理經驗  程式設計 數位邏輯實驗 感測器原理與應用   實習  工業局 108 年度智慧機器人才培育 桃園協易機械 - 沖壓機械智能感測產學專案實習 桃園協易機械實習培訓 - LabVIEW講師    各系所申請人數與錄取人數  109學年度     系所 錄取人數 申請人數 錄取率     台大電機所甲組 20 110 18.</description>
    </item>
    
    <item>
      <title>Anaconda Virtual Environment Use tensorflow-gpu</title>
      <link>https://Offliners.github.io/post/anaconda-virtualenv-use-tensorflow-gpu/</link>
      <pubDate>Tue, 30 Mar 2021 22:19:35 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/anaconda-virtualenv-use-tensorflow-gpu/</guid>
      <description>測試環境 作業系統 : Window 10 64bit
顯卡 : NVIDIA Geforce GTX 1050
事前準備 1. 確認電腦是否有NVIDIA顯卡 可以從裝置管理員中顯示卡看電腦是否有NVIDIA顯卡
2. 查詢此顯卡的計算能力 可從此網址查詢到 : https://developer.nvidia.com/cuda-gpus
建議顯卡計算能力(Compute Capability) 大於3.5以上，使用tensorflow-gpu才會有明顯的成效
安裝步驟 1. 安裝Anaconda 到Anaconda官方網站下載 : https://www.anaconda.com/products/individual
安裝一直next過去就好
2. 打開Anaconda Prompt 3. 建立虛擬環境 在命令列輸入conda create -n test_env pip python=3.6，test_env是環境名稱，可以自己取，然後Python的版本是3.6
這裡輸入y
4. 進入虛擬環境 建立完成後，輸入activate test_env進入虛擬環境
當前方出現自己的環境名稱表示進入成功
5. 安裝tensorflow-gpu 輸入pip install tensorflow-gpu==1.13.1，安裝版本為1.13.1的tensorflow-gpu
6. 安裝 CUDA 10.0 進入此網站 : https://developer.nvidia.com/cuda-toolkit-archive
尋找CUDA 10.0並安裝
一直按下一步就安裝完成
確認是否有加入環境變數，在Window搜尋環境變數
CUDA安裝軟體基本上會自動填入路徑
安裝cuDNN 7.6.5 進入此網站 : https://developer.nvidia.com/rdp/cudnn-archive</description>
    </item>
    
    <item>
      <title>使用 Hugo 在 Github Pages 建立靜態網站</title>
      <link>https://Offliners.github.io/post/hugo-first/</link>
      <pubDate>Tue, 30 Mar 2021 21:19:31 +0800</pubDate>
      
      <guid>https://Offliners.github.io/post/hugo-first/</guid>
      <description>Hugo 簡介 Hugo 是使用Go語言編寫的靜態網頁生成器，基於Go語言開發，沒有像HTML5錯綜複雜的依賴關係。除此之外，還擁有活躍的社群提供多樣主題與完善的功能給使用者，支援多種類的前端語言，整體架構也靈活易懂，最大優點是編譯速度極快，建立網站時間平均不到1秒種，使用者修改時能即時預覽。
Hugo 安裝  MacOS &amp;amp; Linux  MacOS使用者可使用Homebrew安裝
brew install hugo  Window  Window使用者須視使用者環境下載適用的版本 https://github.com/gohugoio/hugo/releases 或者透過chocolatey安裝
choco install hugo -confirm 安裝好後再去設定環境變數，例如: C:\hugo，記得將hugo.exe放進此路徑資料夾中
Hugo 指令測試 安裝好後透過CMD輸入hugo version可查看版本
hugo v0.81.0-59D15C97 linux/amd64 BuildDate=2021-02-19T17:07:12Z VendorInfo=gohugoio 建立 Project 透過CMD輸入
hugo new site mysite 則會在此路徑下建立mysite的資料夾，mysite可換任意名稱 資料夾結構如下:
mysite/ │ ├── archetypes/ │ │ │ └─ default.md # 預設 markdown ├── content/ # 頁面、文章（markdown） ├── data/ # 資料庫 ├── layouts/ # 自定義的樣板 ├── static/ # 靜態資源 ├── themes/ # 網站的主題 └── config.</description>
    </item>
    
  </channel>
</rss>
